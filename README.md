# Bias Mitigation in Machine Learning Models using Explainable AI for Recidivism

## Overview
This project explores **machine learning models** for predicting criminal recidivism (reoffending) using the **ProPublica COMPAS dataset**.  
The key focus is not only accuracy but also **fairness** â€” ensuring models do not discriminate by **race** or **sex**.  

We test multiple models, explain their predictions with **XAI tools (SHAP, Fairlearn)**, and apply **bias mitigation techniques** to improve equity.  

---

## ğŸ” Research Flow

### 1. Dataset
- **ProPublica COMPAS dataset** â€“ widely used for fairness studies in criminal justice.  
- Includes demographics, criminal history, charges, and outcomes.  
- Used because it highlights **real-world bias challenges** in AI.

### 2. Preprocessing
- Dropped irrelevant IDs and cleaned missing data.  
- **One-hot encoded** sex & race â†’ for ML interpretability.  
- Why: Makes raw data usable for models and fairness analysis.  

### 3. Models
We compared **simple, margin-based, and ensemble models**:  
- **Logistic Regression (baseline)** â€“ interpretable, sets a benchmark.  
- **Support Vector Classifier (SVC)** â€“ captures complex boundaries.  
- **Gradient Boosted Classifier (GBC)** â€“ strong with non-linear patterns.  
- **XGBoost (XGB)** â€“ optimized boosting, handles large feature interactions.  
- Why: To see which models balance accuracy and fairness best.  

### 4. Explainability
- **SHAP (feature importance)** â†’ Shows how priors, age, sex, race affect predictions.  
- **Fairlearn (group fairness)** â†’ Measures disparities across groups.  
- Why: Makes models transparent & exposes hidden bias.  

### 5. Bias Mitigation
- **In-processing**: *Adversarial Debiasing* â€“ forces models to predict without relying on sensitive features.  
- **Post-processing**: *ROC, Equalized Odds, CEO* â€“ adjust outputs to reduce unfairness.  
- Why: To reduce discrimination without losing accuracy.  

### 6. Results
- **SVC = Best trade-off**:  
  - Accuracy ~68%  
  - Fairness (Disparate Impact) â‰ˆ 1.0 (ideal)  
- Logistic Regression â†’ high accuracy but high bias.  
- XGBoost â†’ fairer but less accurate.  
- Why: Shows fairness interventions **work without hurting performance**.  

---

## Repository
- `compas-scores-two-years.csv` â€“ dataset.  
- `Final_Processed_Propublica_Compas.ipynb` â€“ preprocessing & EDA.  
- `Model_accuracies_on_is_recid.ipynb` â€“ model training on `is_recid`.  
- `Model_accuracies_on_two_year_recid.ipynb` â€“ model training on `two_year_recid`.  
- `Bias Mitigation final.pdf` â€“ full research paper.  

---

## âš™ï¸ Usage
```bash
pip install pandas numpy scikit-learn xgboost shap fairlearn matplotlib seaborn
